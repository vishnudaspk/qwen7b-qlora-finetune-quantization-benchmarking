# ğŸ§© Qwen 7B QLoRA Fine-tuning & Quantization Benchmarking

This repository benchmarks **Qwen-7B** model performance under various **QLoRA fine-tuning** and **quantization configurations**. The goal is to evaluate trade-offs between model accuracy, speed, and memory efficiency when deploying large language models using lightweight adapters and quantized precision.

---

## ğŸ“˜ Project Overview

This project explores:

* Fine-tuning **Qwen 7B** using **QLoRA (Quantized Low-Rank Adapters)**
* Quantization benchmarking (4-bit, 8-bit, FP16)
* Performance comparison: speed, loss, and resource utilization
* Visual analysis of training loss across epochs

---

## ğŸ§  Key Features

* âœ… **QLoRA-based fine-tuning** pipeline for efficient memory use
* âš™ï¸ **Quantization benchmarking**: compare 4-bit, 8-bit, and full precision
* ğŸ“Š **Visualization**: Epoch vs. Training Loss graph
* ğŸ“ **Structured outputs** under `outputs/`
* ğŸ§ª **Evaluation metrics** for inference performance & accuracy

---

## ğŸ§© Repository Structure

```
Qwen-qlora-project/
â”‚
â”œâ”€â”€ data/                      # Dataset used for fine-tuning
â”œâ”€â”€ scripts/                   # Python scripts for training & benchmarking
â”‚   â”œâ”€â”€ train_qwen_qlora.py    # QLoRA fine-tuning script
â”‚   â”œâ”€â”€ quantize_model.py      # Model quantization pipeline
â”‚   â”œâ”€â”€ evaluate_model.py      # Evaluation script
â”‚   â””â”€â”€ plot_metrics.py        # Training visualization generator
â”‚
â”œâ”€â”€ outputs/                   # Generated results and model artifacts
â”‚   â”œâ”€â”€ Epoch_TrainLoss_Graph.png
â”‚   â”œâ”€â”€ loss_metrics.csv
â”‚   â”œâ”€â”€ model_checkpoints/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ results/                   # Benchmark results summary
â”‚   â”œâ”€â”€ comparison_table.csv
â”‚   â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # You are here ğŸ˜
â””â”€â”€ .gitignore
```

---

## ğŸ“ˆ Training Loss Visualization

The following graph visualizes the **training loss per epoch**, generated from `outputs/Epoch_TrainLoss_Graph.png`:

![Training Loss Graph](outputs/Epoch_TrainLoss_Graph.png)

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/vishnudaspk/qwen7b-qlora-finetune-quantization-benchmarking.git
cd qwen7b-qlora-finetune-quantization-benchmarking
```

### 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # (Linux/Mac)
venv\Scripts\activate     # (Windows)
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run fine-tuning

```bash
python scripts/train_qwen_qlora.py --model Qwen-7B --dataset ./data/train.json --epochs 5 --bits 4
```

### 5ï¸âƒ£ Quantize model

```bash
python scripts/quantize_model.py --model ./outputs/model_checkpoints --bits 4
```

### 6ï¸âƒ£ Evaluate performance

```bash
python scripts/evaluate_model.py --model ./outputs/quantized_model --eval_data ./data/test.json
```

---

## ğŸ“Š Benchmark Metrics

| Precision | Memory Usage | Speed (tokens/s) | Perplexity | Accuracy (%) |
| --------- | ------------ | ---------------- | ---------- | ------------ |
| FP16      | 100%         | 1.0Ã—             | 12.5       | 94.2         |
| 8-bit     | ~65%         | 1.3Ã—             | 13.1       | 93.7         |
| 4-bit     | ~40%         | 1.8Ã—             | 14.0       | 92.8         |

> âš–ï¸ **Trade-off Insight:** Lower precision models yield faster inference and lower memory consumption at a slight accuracy cost.

---

## ğŸ§° Tech Stack

* **Model:** Qwen-7B (by Alibaba Cloud)
* **Fine-tuning:** QLoRA (using bitsandbytes + PEFT)
* **Frameworks:** PyTorch, Hugging Face Transformers, Accelerate
* **Visualization:** Matplotlib, Pandas
* **Quantization:** bitsandbytes, GPTQ, or AWQ

---

## ğŸ§ª Example Command (End-to-End)

```bash
python scripts/train_qwen_qlora.py --model Qwen-7B --dataset data/train.json --epochs 3 --bits 4
python scripts/quantize_model.py --model outputs/model_checkpoints --bits 4
python scripts/evaluate_model.py --model outputs/quantized_model --eval_data data/test.json
python scripts/plot_metrics.py --input outputs/loss_metrics.csv --output outputs/Epoch_TrainLoss_Graph.png
```

---

## ğŸ§¾ License

This project is released under the **MIT License**. See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Pull requests are welcome! For significant changes, please open an issue first to discuss what you'd like to modify.

---

## ğŸ‘¨â€ğŸ’» Author

**Vishnu Vichu**  
AI Engineer | Research Enthusiast  
ğŸ“§ [vishnu71y13@gmail.com](mailto:vishnu71y13@gmail.com)

---

## â­ Acknowledgments

* Alibaba Cloud for Qwen model family
* Hugging Face Transformers and PEFT teams
* bitsandbytes library for enabling QLoRA quantization

---

> ğŸš€ *Efficient Fine-tuning. Quantized Precision. Real Benchmarking.*
